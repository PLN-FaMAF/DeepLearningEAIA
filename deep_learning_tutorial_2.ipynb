{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Express Deep Learning in Python\n",
    "\n",
    "## Advanced Layers\n",
    "\n",
    "The `Dense` layer is only one of the possible core layers of Keras. `Dense` is a *forward* layer, this are the ones that take an input and do some transformation on it (in this case a matrix multiplication).\n",
    "\n",
    "Other important layers to consider are: activation layers, regularization layers, dropout layers, convolutional layers, pooling layers, recurrent layers, normalization layers, embedding layers, noise layers, etc.\n",
    "\n",
    "For this tutorial we will focus on some layers to aid in the tuning of the network: activations, regularizers and dropout; as well as the layers needed to design convolutional neural networks: convolutional and pooling layers.\n",
    "\n",
    "We will point out other tutorials and examples to learn about the other kind of layers at the end of this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras import regularizers\n",
    "from keras.layers import Activation, ActivityRegularization, Dense, Dropout\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "\n",
    "A neural network classifier with linear activations has no more *representation* power than a logistic regression classifier. In order to express non-linearity with a neural network model a non-linear function is needed as activation function for each neuron.\n",
    "\n",
    "One simple activation function to use is the **sigmoid (or logistic) function**, the same one used in the logistic regression algorithm, which restricts the output value to be between zero and one. This was one of the most common nonlinearities used as activation function in some of the *first versions* of neural networks. There are however other possibilities (all the following available in Keras, but there are more which can be adapted):\n",
    "\n",
    "* rectified linear unit (ReLU)\n",
    "* tanh\n",
    "* hard sigmoid\n",
    "* softsign\n",
    "* softplus\n",
    "* exponential linear unit (elu)\n",
    "* scaled exponential linear unit (selu)\n",
    "* leaky rectifier linear unit (Leaky ReLU)\n",
    "* parametric rectified linear unit (PReLU)\n",
    "\n",
    "Of these, the one most used in the present state-of-the-art neural networks classifiers is the **ReLU**, because tipically learns much faster in networks with many layers [1].\n",
    "\n",
    "There is another activation layer which is the **SoftMax** activation. This is generally used as the last activation layer, i.e. as the output of the network. This function, also known as *normalized exponential function* is a generalization of the logistic function that \"squashes\" a K-dimensional vector ${\\displaystyle \\mathbf {z}}$ of arbitrary real values to a K-dimensional vector ${\\displaystyle \\sigma (\\mathbf {z} )}$ of real values in the range [0, 1] that add up to 1.\n",
    "\n",
    "### Activation Functions in Keras\n",
    "\n",
    "Keras provides two ways to define an activation function. Any method is equally valid.\n",
    "\n",
    "#### Activation as a parameter of a forward layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(784,), activation='relu'))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation as a layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(784,)))\n",
    "model.add(Activation('tanh'))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Activation from a TensorFlow function\n",
    "\n",
    "In the previous examples we used some of the available functions in the Keras library.\n",
    "\n",
    "We can also use an element-wise TensorFlow function as activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape(784,),\n",
    "                activation=K.sigmoid))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularizers\n",
    "\n",
    "Regularizers allow to apply penalties on layer parameters or layer activity during optimization. These penalties are incorporated in the loss function that the network optimizes. The penalties are applied on a per-layer basis.\n",
    "\n",
    "The regularizers can be applied to three parameters:\n",
    "\n",
    "* Weight/kernel matrix regularization: Applies the regularizer function to the weight matrix (called kernel matrix in Keras documentation).\n",
    "* Bias regularization: Applies the regularizer to the bias vector.\n",
    "* Activity regularizer: Applies the regularizer to the output (i.e. the activation function).\n",
    "\n",
    "There are three possible penalties to apply as regularizers already present in Keras (but the API permits the definition of a custom regularizer) [2]: l1, l2 and elasticnet.\n",
    "\n",
    "### Regularizers in Keras\n",
    "\n",
    "As with activation functions, there are two ways to use a regularizer in keras. Although not for all the parameters.\n",
    "\n",
    "#### Regularization as parameter of a layer\n",
    "\n",
    "This is the most practical way and the only one which allows the individual regularization of each available parameter.\n",
    "\n",
    "The regularizer is given as a parameter of the layers (e.g. `Dense`):\n",
    "\n",
    "* `kernel_regularizer`\n",
    "* `bias_regularizer`\n",
    "* `activity_regularizer`\n",
    "\n",
    "The available penalties for this case are:\n",
    "\n",
    "* `keras.regularizers.l1`\n",
    "* `keras.regularizers.l2`\n",
    "* `keras.regularizers.l1_l2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(784,),\n",
    "                activation='relu',\n",
    "                kernel_regularizer=regularizers.l2(0.01),\n",
    "                activity_regularizer=regularizers.l1(0.01)))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regularization as a layer\n",
    "\n",
    "The core layer `ActivityRegularization` is another way to apply regularization, in this case (as the name indicates), only for the activation function (not for the weight matrix or the bias vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(784,), activation='relu'))\n",
    "model.add(ActivityRegularization(l1=0.01, l2=0.1))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "This are special layers useful for regularization which randomly drop (i.e. set to zero) units of the neural network during training. This prevents units from co-adapting too much to the input [3].\n",
    "\n",
    "Keras has a special layer which can be added to a sequential model which takes a value `rate`, between 0 and 1, and sets the fraction given by the value to 0 during training of the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(784,), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the model: loss functions and optimizers\n",
    "\n",
    "Once the model's architecture is defined, i.e. all the layers are given with their respective activation functions and regularization parameters, the model needs to be compiled in order to use it. Remember Keras is an abstraction layer over another abstraction that is the backend, TensorFlow in this case.\n",
    "\n",
    "When compiling a model there are two important parameters: the loss function and the optimizer algorithm.\n",
    "\n",
    "### Loss function\n",
    "\n",
    "Also know as the *objective function*, is the function we want to optimize when training the algorithm (that is find the minimum of the loss function). Depending on the task (whether it is classification or regression), and some other parameters, the objective function can change. Two of the most popular objective functions are the **mean squared error** for regression and **categorical crossentropy** for classification. Keras bring a number of different loss functions already available [4], but for this course we will be using only the *categorical crossentropy*.\n",
    "\n",
    "#### Categorical format\n",
    "\n",
    "In case of using a loss function for classification (e.g. the categorical crossentropy) having more than 2 classes, Keras requires the targets to be in categorical format (e.g. if you have 10 classes, the target for each sample should be a 10-dimensional vector that is all-zeros expect for a 1 at the index corresponding to the class of the sample). In order to convert integer targets into categorical targets, you can use the Keras utility `keras.utils.np_utils.to_categorical`.\n",
    "\n",
    "### Optimizer\n",
    "\n",
    "The optimizer algorithm is the way to find the minimum values to the loss function. As with loss functions, there are many available optimizers already packaged with Keras. One of the most popular algorithms is **stochastic gradient descent** (or SGD) optimizer, which is also one of the simplest to understand. However, in this tutorial we will be using mostly the --Adam-- **REVISAR DE ACUERDO A RESULTADOS** optimizer which gives the best results.\n",
    "\n",
    "### Compiling a model in Keras\n",
    "\n",
    "In Keras, a model can be compiled with the method `.compile()` in a model. The method takes two parameters: `loss` and `optimizer`. The parameters can either be instances of a loss function (e.g. `keras.losses.hinge_loss`) or an optimizer (e.g. `keras.optimizers.RMSprop`) or a string calling the loss function or optimizer by the name.\n",
    "\n",
    "The main difference between using an instance and a string is that in the latter case the loss function or optimizer will be used with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- [1] LeCun, Yann, Yoshua Bengio, and Geoffrey Hinton. \"Deep learning.\" Nature 521, no. 7553 (2015): 436-444.\n",
    "- [2] \"Developing new regularizers\". Keras Documentation. https://keras.io/regularizers/\n",
    "- [3] Srivastava, Nitish, Geoffrey E. Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. \"Dropout: a simple way to prevent neural networks from overfitting.\" Journal of machine learning research 15, no. 1 (2014): 1929-1958. Harvard\t\n",
    "- [4] \"Available loss functions\". Keras documentation. https://keras.io/losses/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
