{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Express Deep Learning in Python\n",
    "\n",
    "## How fast can you build a MLP?\n",
    "\n",
    "In this tutorial we aim to give you some notions on how Deep Learning algorithms are implemented and the key points to use them and not get burned in the process. We will center on how to translate the theory of neural networks easily into efficient code with as little effort as possible. We won't teach you how to implement the complex algorithms involved, but rather how to ensemble the available parts to get the classification pipeline you want. You don't need to be an expert in machine learning or deep learning to understand this content, but some prior background is required. Also, we count on you knowing how to program in Python and how to use Jupyter Notebooks. \n",
    "\n",
    "In this first part we will see how to implement the basic components of a MultiLayer Perceptron (MLP) classifier, most commonly known as Neural Network. We will be working with the Keras: a very simple library for deep learning.\n",
    "\n",
    "At this point, you may know how machine learning in general is applied and have some intuitions about how deep learning works, and more importantly, why it works. Now it's time to make some experiments, and for that you need to be as quick and flexible as possible. Keras is an idea tool for prototyping and doing your first approximations to a Machine Learning problem. On the one hand, Keras is integrated with two very powerfull backends that support GPU computations, Tensorflow and Theano. On the other hand, it has a level of abstraction high enough to be simple to understand and easy to use. For example, it uses a very similar interface to the sklearn library that you have seen before, with fit and predict methods.\n",
    "\n",
    "Now let's get to work with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - The libraries\n",
    "Firts let's check we have installed everything we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - The dataset\n",
    "\n",
    "For this quick tutorial we will use the (very popular) [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This is a dataset of 70K images of handwritten digits. Our task is to recognize which digits is displayed in the image: a classification problem. You have seen in previous courses how to train and evaluate a classifier, so we wont talk in further details about supervised learning.\n",
    "\n",
    "The input to the MLP classifier are going to be images of 28x28 pixels represented as matrixes. The output will be one of ten classes (0 to 9), representing the predicted number written in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "TRAIN_EXAMPLES = 60000\n",
    "TEST_EXAMPLES = 10000\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the dataset to convert the examples from 2D matrixes to 1D arrays.\n",
    "x_train = x_train.reshape(60000, 28*28)\n",
    "x_test = x_test.reshape(10000, 28*28)\n",
    "\n",
    "# to make quick runs, select a smaller set of images.\n",
    "x_train = x_train[:TRAIN_EXAMPLES].astype('float32')\n",
    "x_test = x_train[:TEST_EXAMPLES].astype('float32')\n",
    "\n",
    "# normalize the input\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train[:TRAIN_EXAMPLES], num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test[:TEST_EXAMPLES], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - The model\n",
    "\n",
    "The concept of Deep Learning is very broad, but the core of it is the use of classifiers with multiple hidden layer of neurons, or smaller classifiers. We all know the classical image of the simplest possible possible deep model: a neural netowrk with a single hidden layer. \n",
    "\n",
    "![Neural Network](files/NeuralNetwork.png \"Title\")\n",
    "\n",
    "credits http://www.extremetech.com/wp-content/uploads/2015/07/NeuralNetwork.png\n",
    "\n",
    "In theory, this model can represent any function ***TODO*** add a citation here. We will see how to implement this network in Keras, and during the second part of this tutorial how to add more features to create a deep and powerful classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Deep Learning models are concatenations of Layers. This is represented in Keras with the Sequential model. We create the Sequential instance as an \"empty carcass\" and then we fill it with different layers. \n",
    "\n",
    "The most basic type of Layer is the Dense layer, where each neuron in the input is connected to each neuron in the following layer, like we can see in the image above. Internally, a Dense layer has two variables: a matrix of weights and a vector of bias, but you beauty of Keras is that you don't need to worry about that. All the variables will be correctly created, initialized, trained and possibly regularized for you.\n",
    "\n",
    "Each layer needs to know or be able to calculate al least three things:\n",
    "\n",
    "* The size of the input: the number of neurons in the incoming layer. For the first layer this corresponds to the size of each example in our dataset. The next layers can calculate their input size using the output of the previous layer, so we generally don't need to tell them this.\n",
    "* The type of activation: this is the function that is applied to the output of each neuron. Will talk in detail about this later.\n",
    "* The size of the output: the number of neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input to hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# Hidden to output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully build a Neural Network! We can print a description of our architecture using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling a model in Keras\n",
    "\n",
    "A very appealing aspect of Deep Learning frameworks is that they solve the implementation of complex algorithms such as Backpropagation. For those with some numerical optimization notions, minimization algorithms often involve the calculation of first defivatives. Neural Networks are huge functions full of non-linearities, and differentiating them is a... nightmare. For this reason, models need to be \"compiled\". In this stage, the backend builds complex computational graphs, and we don't have to worry about derivatives or gradients.\n",
    "\n",
    "In Keras, a model can be compiled with the method `.compile()`. The method takes two parameters: loss and optimizer. The **loss** is the function that calculates how much error we have in each prediction example, and there are a lot of implemented alternatives ready to use. We will talk more about this, for now we use the standard categorical crossentropy. As you can see, we can simply pass a string with the name of the function and Keras will find the implementation for us. We could, as well, pass a custom function as parameter.\n",
    "\n",
    "The **optimizer** is the algorithm to minimize the value of the loss function. Again, Keras has many optimizers available. The basic one is the Stochastic Gradient Descent.\n",
    "\n",
    "We pass a third argument to the `compile` method: the metric. **Metrics** are measures or statistics that allows us to keep track of the classifier's performance. It's similar to the loss, but the results of the metrics are not use by the optimization algorithm. Also, we can monitor how other performance metrics vary when the loss is optimized and detect anomalies. **TODO** metrics are calculated on the validation dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL] We can now visualize the architecture of our model using the `vis_util` tools. It's a very schematic view, but you can check it's not that different from the image we saw above (and that we intended to replicate).\n",
    "\n",
    "If you can't execute this step don't worry, you can still finish the tutorial. This step requires graphviz and pydotplus libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 171.00 191.00\" width=\"171pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 167,-187 167,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140359084249040 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140359084249040</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 163,-182.5 163,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-160.8\">dense_1_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140359084248424 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140359084248424</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-73.5 30.5,-109.5 132.5,-109.5 132.5,-73.5 30.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-87.8\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 140359084249040&#45;&gt;140359084248424 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140359084249040-&gt;140359084248424</title>\n",
       "<path d=\"M81.5,-146.313C81.5,-138.289 81.5,-128.547 81.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-119.529 81.5,-109.529 78.0001,-119.529 85.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140359084248480 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140359084248480</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-0.5 30.5,-36.5 132.5,-36.5 132.5,-0.5 30.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-14.8\">dense_2: Dense</text>\n",
       "</g>\n",
       "<!-- 140359084248424&#45;&gt;140359084248480 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140359084248424-&gt;140359084248480</title>\n",
       "<path d=\"M81.5,-73.3129C81.5,-65.2895 81.5,-55.5475 81.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-46.5288 81.5,-36.5288 78.0001,-46.5289 85.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is compiled, everything is ready to train the classifier. Keras' `Sequential` model has a similar interface as the sklearn library that you have seen before, with `fit` and `predict` methods. As usual, we need to pass our training examples and their corresponding labels. Other parameters needed to train a neural network is the size of the batch and the number of epochs. We have two ways of specifying a validation dataset: we can pass the tuple of values and labels directly with the `validation_data` parameter, or we can pass a proportion to the `validation_split` argument and Keras will split the training dataset for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.1381 - acc: 0.9595 - val_loss: 9.5961 - val_acc: 0.1002\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 4s - loss: 0.0795 - acc: 0.9764 - val_loss: 10.6274 - val_acc: 0.0996\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0543 - acc: 0.9838 - val_loss: 11.1379 - val_acc: 0.0998\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 5s - loss: 0.0379 - acc: 0.9891 - val_loss: 11.8913 - val_acc: 0.1000\n",
      "Epoch 5/10\n",
      "18176/60000 [========>.....................] - ETA: 3s - loss: 0.0264 - acc: 0.9930"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained our model!\n",
    "\n",
    "**TODO** add more about the metrics that we can see in the logs and why they are useful.\n",
    "\n",
    "Keras gives us a very useful method to evaluate the current performance called `evaluate` (surprise!). Evaluate will return the value of the loss function and all the metrics that we pass to the model when calling compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 10.378407814\n",
      "Test accuracy: 0.095\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using only 10 training epochs we don't get a very surprising accuracy in the validation and test dataset. However, we get nearly perfect accuracy in the training dataset. In short, the model is overfitting. This may be happening because our model is trained with very few epochs, but most likely it happens because our model is too simple and can't generalize to unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 7680/10000 [======================>.......] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_palette('colorblind')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-30ad52d9079e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m figure = sns.heatmap(matrix, annot=True, fmt=\"d\", linewidths=.5,\n\u001b[1;32m      6\u001b[0m                      \u001b[0mxticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0myticklabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                      cmap=sns.cubehelix_palette(8,  as_cmap=True))\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m# plt.show()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mheatmap\u001b[0;34m(data, vmin, vmax, cmap, center, robust, annot, fmt, annot_kws, linewidths, linecolor, cbar, cbar_kws, cbar_ax, square, xticklabels, yticklabels, mask, ax, **kwargs)\u001b[0m\n\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msquare\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_aspect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"equal\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m     \u001b[0mplotter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcbar_ax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36mplot\u001b[0;34m(self, ax, cax, kws)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Annotate the cells with the formatted values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotate_heatmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmesh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[0;31m# Invert the y axis to show the plot in matrix form\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/seaborn/matrix.py\u001b[0m in \u001b[0;36m_annotate_heatmap\u001b[0;34m(self, ax, mesh)\u001b[0m\n\u001b[1;32m    240\u001b[0m                                        self.annot_data.flat):\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                 \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelative_luminance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                 \u001b[0mtext_color\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\".15\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ml\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m.408\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"{:\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfmt\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/seaborn/utils.py\u001b[0m in \u001b[0;36mrelative_luminance\u001b[0;34m(color)\u001b[0m\n\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m     \"\"\"\n\u001b[0;32m--> 533\u001b[0;31m     \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorConverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m     \u001b[0mrgb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m.03928\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrgb\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m12.92\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrgb\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m1.055\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2.4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0mlum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m.2126\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.7152\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m.0722\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(arg, alpha)\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mthen\u001b[0m \u001b[0man\u001b[0m \u001b[0mempty\u001b[0m \u001b[0marray\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mreturned\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mSame\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0man\u001b[0m \u001b[0mempty\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    335\u001b[0m         \"\"\"\n\u001b[0;32m--> 336\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mto_rgba_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba_array\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mto_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mValueError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36mto_rgba\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Not in cache, or unhashable.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m         \u001b[0mrgba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_rgba_no_colorcycle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0m_colors_full_map\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrgba\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/keras/lib/python3.5/site-packages/matplotlib/colors.py\u001b[0m in \u001b[0;36m_to_rgba_no_colorcycle\u001b[0;34m(c, alpha)\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid RGBA argument: {!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_c\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# Return a tuple to prevent the cached value from being modified.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RGBA sequence should have length 3 or 4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from sklearn.metrics import confusion_matrix\n",
    "matrix = confusion_matrix(numpy.argmax(x_test, 1), prediction)\n",
    "colormap = plt.cm.cubehelix_r\n",
    "figure = sns.heatmap(matrix, annot=True, fmt=\"d\", linewidths=.5,\n",
    "                     xticklabels=range(10), yticklabels=range(10),\n",
    "                     cmap=sns.cubehelix_palette(8,  as_cmap=True))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Practical task\n",
    "\n",
    "Experiment modifying the above model by adding more dense layers, changing the number of neurons and examine how the validation and testing performance changes. Is it useful to add other metrics like the f1 score? What about printing the confusion matrixes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
