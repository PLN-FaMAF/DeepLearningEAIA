{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional, only if you installed Seaborn\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Express Deep Learning in Python - Part 1\n",
    "\n",
    "Do you have everything ready? Check the part 0!\n",
    "\n",
    "## How fast can you build a MLP?\n",
    "\n",
    "In this first part we will see how to implement the basic components of a MultiLayer Perceptron (MLP) classifier, most commonly known as Neural Network. We will be working with the [Keras](https://keras.io/): a very simple library for deep learning.\n",
    "\n",
    "At this point, you may know how machine learning in general is applied and have some intuitions about how deep learning works, and more importantly, why it works. Now it's time to make some experiments, and for that you need to be as quick and flexible as possible. Keras is an idea tool for prototyping and doing your first approximations to a Machine Learning problem. On the one hand, Keras is integrated with two very powerfull backends that support GPU computations, Tensorflow and Theano. On the other hand, it has a level of abstraction high enough to be simple to understand and easy to use. For example, it uses a very similar interface to the sklearn library that you have seen before, with fit and predict methods.\n",
    "\n",
    "Now let's get to work with an example:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - The libraries\n",
    "Firts let's check we have installed everything we need for this tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - The dataset\n",
    "\n",
    "For this quick tutorial we will use the (very popular) [MNIST dataset](http://yann.lecun.com/exdb/mnist/). This is a dataset of 70K images of handwritten digits. Our task is to recognize which digits is displayed in the image: a **classification** problem. You have seen in previous courses how to train and evaluate a classifier, so we wont talk in further details about supervised learning.\n",
    "\n",
    "The input to the MLP classifier are going to be images of 28x28 pixels represented as matrixes. The output will be one of ten classes (0 to 9), representing the predicted number written in the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "TRAIN_EXAMPLES = 20000\n",
    "TEST_EXAMPLES = 5000\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# reshape the dataset to convert the examples from 2D matrixes to 1D arrays.\n",
    "x_train = x_train.reshape(60000, 28*28)\n",
    "x_test = x_test.reshape(10000, 28*28)\n",
    "\n",
    "# to make quick runs, select a smaller set of images.\n",
    "x_train = x_train[numpy.random.choice(x_train.shape[0], TRAIN_EXAMPLES, replace=False), :].astype('float32')\n",
    "x_test = x_train[numpy.random.choice(x_test.shape[0], TEST_EXAMPLES, replace=False), :].astype('float32')\n",
    "\n",
    "# normalize the input\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train[:TRAIN_EXAMPLES], num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test[:TEST_EXAMPLES], num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - The model\n",
    "\n",
    "The concept of Deep Learning is very broad, but the core of it is the use of classifiers with multiple hidden layer of neurons, or smaller classifiers. We all know the classical image of the simplest possible possible deep model: a neural network with a single hidden layer. \n",
    "\n",
    "![Neural Network](files/NeuralNetwork.png \"Basic Neural Network Architecture\")\n",
    "\n",
    "credits http://www.extremetech.com/wp-content/uploads/2015/07/NeuralNetwork.png\n",
    "\n",
    "In theory, this model can represent any function ***TODO*** add a citation here. We will see how to implement this network in Keras, and during the second part of this tutorial how to add more features to create a deep and powerful classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, Deep Learning models are concatenations of Layers. This is represented in Keras with the Sequential model. We create the Sequential instance as an \"empty carcass\" and then we fill it with different layers. \n",
    "\n",
    "The most basic type of Layer is the Dense layer, where each neuron in the input is connected to each neuron in the following layer, like we can see in the image above. Internally, a Dense layer has two variables: a matrix of weights and a vector of bias, but the beauty of Keras is that you don't need to worry about that. All the variables will be correctly created, initialized, trained and possibly regularized for you.\n",
    "\n",
    "Each layer needs to know or be able to calculate al least three things:\n",
    "\n",
    "* The size of the input: the number of neurons in the incoming layer. For the first layer this corresponds to the size of each example in our dataset. The next layers can calculate their input size using the output of the previous layer, so we generally don't need to tell them this.\n",
    "* The type of activation: this is the function that is applied to the output of each neuron. Will talk in detail about this later.\n",
    "* The size of the output: the number of neurons in the next layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# Input to hidden layer\n",
    "model.add(Dense(512, activation='relu', input_shape=(784,)))\n",
    "# Hidden to output layer\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have successfully build a Neural Network! We can print a description of our architecture using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 512)               401920    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 407,050\n",
      "Trainable params: 407,050\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling a model in Keras\n",
    "\n",
    "A very appealing aspect of Deep Learning frameworks is that they solve the implementation of complex algorithms such as Backpropagation. For those with some numerical optimization notions, minimization algorithms often involve the calculation of first defivatives. Neural Networks are huge functions full of non-linearities, and differentiating them is a... nightmare. For this reason, models need to be \"compiled\". In this stage, the backend builds complex computational graphs, and we don't have to worry about derivatives or gradients.\n",
    "\n",
    "In Keras, a model can be compiled with the method `.compile()`. The method takes two parameters: loss and optimizer. The **loss** is the function that calculates how much error we have in each prediction example, and there are a lot of implemented alternatives ready to use. We will talk more about this, for now we use the standard categorical crossentropy. As you can see, we can simply pass a string with the name of the function and Keras will find the implementation for us.\n",
    "\n",
    "The **optimizer** is the algorithm to minimize the value of the loss function. Again, Keras has many optimizers available. The basic one is the Stochastic Gradient Descent.\n",
    "\n",
    "We pass a third argument to the `compile` method: the metric. **Metrics** are measures or statistics that allows us to keep track of the classifier's performance. It's similar to the loss, but the results of the metrics are not use by the optimization algorithm. Besides, metrics are always comparable, while the loss function can take random values depending on your problem.\n",
    "\n",
    "Keras will calculate metrics and loss both on the training and the validation dataset. That way,  we can monitor how other performance metrics vary when the loss is optimized and detect anomalies like overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL] We can now visualize the architecture of our model using the `vis_util` tools. It's a very schematic view, but you can check it's not that different from the image we saw above (and that we intended to replicate).\n",
    "\n",
    "If you can't execute this step don't worry, you can still finish the tutorial. This step requires graphviz and pydotplus libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"191pt\" viewBox=\"0.00 0.00 171.00 191.00\" width=\"171pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"white\" points=\"-4,4 -4,-187 167,-187 167,4 -4,4\" stroke=\"none\"/>\n",
       "<!-- 140029082030208 -->\n",
       "<g class=\"node\" id=\"node1\"><title>140029082030208</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 163,-182.5 163,-146.5 0,-146.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-160.8\">dense_3_input: InputLayer</text>\n",
       "</g>\n",
       "<!-- 140029081839768 -->\n",
       "<g class=\"node\" id=\"node2\"><title>140029081839768</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-73.5 30.5,-109.5 132.5,-109.5 132.5,-73.5 30.5,-73.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-87.8\">dense_3: Dense</text>\n",
       "</g>\n",
       "<!-- 140029082030208&#45;&gt;140029081839768 -->\n",
       "<g class=\"edge\" id=\"edge1\"><title>140029082030208-&gt;140029081839768</title>\n",
       "<path d=\"M81.5,-146.313C81.5,-138.289 81.5,-128.547 81.5,-119.569\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-119.529 81.5,-109.529 78.0001,-119.529 85.0001,-119.529\" stroke=\"black\"/>\n",
       "</g>\n",
       "<!-- 140029081839880 -->\n",
       "<g class=\"node\" id=\"node3\"><title>140029081839880</title>\n",
       "<polygon fill=\"none\" points=\"30.5,-0.5 30.5,-36.5 132.5,-36.5 132.5,-0.5 30.5,-0.5\" stroke=\"black\"/>\n",
       "<text font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"81.5\" y=\"-14.8\">dense_4: Dense</text>\n",
       "</g>\n",
       "<!-- 140029081839768&#45;&gt;140029081839880 -->\n",
       "<g class=\"edge\" id=\"edge2\"><title>140029081839768-&gt;140029081839880</title>\n",
       "<path d=\"M81.5,-73.3129C81.5,-65.2895 81.5,-55.5475 81.5,-46.5691\" fill=\"none\" stroke=\"black\"/>\n",
       "<polygon fill=\"black\" points=\"85.0001,-46.5288 81.5,-36.5288 78.0001,-46.5289 85.0001,-46.5288\" stroke=\"black\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training\n",
    "Once the model is compiled, everything is ready to train the classifier. Keras' `Sequential` model has a similar interface as the sklearn library that you have seen before, with `fit` and `predict` methods. As usual, we need to pass our training examples and their corresponding labels. Other parameters needed to train a neural network is the size of the batch and the number of epochs. We have two ways of specifying a validation dataset: we can pass the tuple of values and labels directly with the `validation_data` parameter, or we can pass a proportion to the `validation_split` argument and Keras will split the training dataset for us.\n",
    "\n",
    "To correctly train our model we need to pass two important parameters to the fit function:\n",
    " * **batch_size**: is the number of examples to use in each \"minibatch\" iteration of the Stochastic Gradient Descent algorithm. This is necessary for most optimization algorithms. The size of the batch is important because it defines how fast the algorithm will perform each iteration and also how much memory will be used to load each batch (possibly in the GPU).\n",
    " * **epochs**: is the number of passes through the entire dataset. We need enough epochs for the classifier to converge, but we need to stop before the classifier starts overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.3254 - acc: 0.1019 - val_loss: 2.3072 - val_acc: 0.1086\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 1s - loss: 2.2906 - acc: 0.1266 - val_loss: 2.3201 - val_acc: 0.0972\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.2743 - acc: 0.1443 - val_loss: 2.3232 - val_acc: 0.1020\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.2524 - acc: 0.1659 - val_loss: 2.3375 - val_acc: 0.1048\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.2239 - acc: 0.1809 - val_loss: 2.3507 - val_acc: 0.1006\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.1919 - acc: 0.2026 - val_loss: 2.3683 - val_acc: 0.0980\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.1507 - acc: 0.2215 - val_loss: 2.3961 - val_acc: 0.1082\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.1029 - acc: 0.2478 - val_loss: 2.4292 - val_acc: 0.1046\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 2s - loss: 2.0490 - acc: 0.2720 - val_loss: 2.4676 - val_acc: 0.1068\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 2s - loss: 1.9869 - acc: 0.3097 - val_loss: 2.5086 - val_acc: 0.1048\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "          verbose=1, validation_data=(x_test, y_test));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have trained our model!\n",
    "\n",
    "Additionally, Keras has printed out a lot of information of the training, thanks to the parameter `verbose=1` that we passed to the fit function. We can see how many time it took in each iteration, and the value of the loss and metrics in the training and the validation dataset. The same information is stored in the output of the fit method, which sadly it's not well documented. We can see it in a pretty table with pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>acc</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.10195</td>\n",
       "      <td>2.325419</td>\n",
       "      <td>0.1086</td>\n",
       "      <td>2.307201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.12660</td>\n",
       "      <td>2.290577</td>\n",
       "      <td>0.0972</td>\n",
       "      <td>2.320071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.14430</td>\n",
       "      <td>2.274311</td>\n",
       "      <td>0.1020</td>\n",
       "      <td>2.323224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.16595</td>\n",
       "      <td>2.252435</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>2.337523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.18095</td>\n",
       "      <td>2.223880</td>\n",
       "      <td>0.1006</td>\n",
       "      <td>2.350725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.20260</td>\n",
       "      <td>2.191887</td>\n",
       "      <td>0.0980</td>\n",
       "      <td>2.368346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.22150</td>\n",
       "      <td>2.150686</td>\n",
       "      <td>0.1082</td>\n",
       "      <td>2.396130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.24780</td>\n",
       "      <td>2.102903</td>\n",
       "      <td>0.1046</td>\n",
       "      <td>2.429248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.27200</td>\n",
       "      <td>2.049002</td>\n",
       "      <td>0.1068</td>\n",
       "      <td>2.467600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.30970</td>\n",
       "      <td>1.986886</td>\n",
       "      <td>0.1048</td>\n",
       "      <td>2.508576</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       acc      loss  val_acc  val_loss\n",
       "0  0.10195  2.325419   0.1086  2.307201\n",
       "1  0.12660  2.290577   0.0972  2.320071\n",
       "2  0.14430  2.274311   0.1020  2.323224\n",
       "3  0.16595  2.252435   0.1048  2.337523\n",
       "4  0.18095  2.223880   0.1006  2.350725\n",
       "5  0.20260  2.191887   0.0980  2.368346\n",
       "6  0.22150  2.150686   0.1082  2.396130\n",
       "7  0.24780  2.102903   0.1046  2.429248\n",
       "8  0.27200  2.049002   0.1068  2.467600\n",
       "9  0.30970  1.986886   0.1048  2.508576"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "pandas.DataFrame(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this useful? This will give you an insight on how well your network is optimizing the loss, and how much it's actually learning. When training, you need to keep track of two things:\n",
    "\n",
    "* Your network is actually learning. This means your training loss is decreasing in average. If it's going up or it's stuck for more than a couple of epochs is safe to stop you training and try again.\n",
    "* You network is not overfitting. It's normal to have a gap between the validation and the training metrics, but they should decrease more or less at the same rate. If you see that your metrics for training are getting better but your validation metrics are getting worse, it is also a good point to stop and fix your overfitting problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras gives us a very useful method to evaluate the current performance called `evaluate` (surprise!). Evaluate will return the value of the loss function and all the metrics that we pass to the model when calling compile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 13.1680129364\n",
      "Test accuracy: 0.1006\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, using only 10 training epochs we don't get a very surprising accuracy in the validation and test dataset. However, we get nearly perfect accuracy in the training dataset. In short, the model is overfitting. We can take a better look at the behaviour of our classifier to see if we can diagnose something. Let's print out the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9952/10000 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "prediction = model.predict_classes(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "sns.set_style('white')\n",
    "sns.set_palette('colorblind')\n",
    "colormap = plt.cm.cubehelix_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVkAAAD1CAYAAAD6W4b6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f17e870d2e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "matrix = confusion_matrix(numpy.argmax(y_test, 1), prediction)\n",
    "figure = sns.heatmap(matrix, fmt=\"d\", linewidths=0,\n",
    "                     xticklabels=range(10), yticklabels=range(10),\n",
    "                     cmap=sns.cubehelix_palette(8,  as_cmap=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the model is assigning classes on what appears to be a random manner. There is no clear pattern or majoritary task. This may be happening because our model is trained with very few epochs, but most likely it happens because our model is too simple and can't generalize to unseen data. In the following part of the tutorial, we will see the details of more complex components of neural classifiers and how to use them to build a more powerful classifier."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:keras]",
   "language": "python",
   "name": "conda-env-keras-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
